{"cells":[{"cell_type":"markdown","metadata":{"id":"5nEHdYcCt-m3"},"source":["### Author: [@SauravMaheshkar](https://twitter.com/MaheshkarSaurav)"]},{"cell_type":"markdown","metadata":{"id":"HL-CfT0quEz8"},"source":["# Packages 📦 and Basic Setup\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"rltlpuWwuNzg"},"source":["## Install Packages"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"executionInfo":{"elapsed":7831,"status":"ok","timestamp":1680843174871,"user":{"displayName":"","userId":""},"user_tz":-540},"id":"HxKLQT_ot6rH","outputId":"4e9e9a0a-1213-4466-860c-802c1eca0e8e"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torchtext import data\n","from torch.autograd import Variable\n","from torchtext import datasets\n","from torchtext.vocab import Vectors, GloVe"]},{"cell_type":"markdown","metadata":{"id":"Drpvydbp-GkP"},"source":["## Project Configuration using `wandb.config`"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"executionInfo":{"elapsed":631,"status":"error","timestamp":1680842959844,"user":{"displayName":"","userId":""},"user_tz":-540},"id":"UihlfsH3-JRq","outputId":"e07d2717-bfc5-4256-9e5e-ef923035a323"},"outputs":[{"ename":"Error","evalue":"You must call wandb.init() before wandb.config.learning_rate","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# Feel free to change these and experiment !!\u001b[39;00m\n\u001b[1;32m      8\u001b[0m config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n\u001b[0;32m----> 9\u001b[0m config\u001b[39m.\u001b[39mlearning_rate \u001b[39m=\u001b[39m \u001b[39m2e-5\u001b[39m\n\u001b[1;32m     10\u001b[0m config\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[1;32m     11\u001b[0m config\u001b[39m.\u001b[39moutput_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n","File \u001b[0;32m~/miniconda/envs/wandb/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:21\u001b[0m, in \u001b[0;36mPreInitObject.__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m, value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m         \u001b[39mraise\u001b[39;00m wandb\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou must call wandb.init() before \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(\u001b[39mself\u001b[39m, key, value)\n","\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.config.learning_rate"]}],"source":["import os\n","import wandb\n","\n","# Paste your api key here\n","os.environ[\"WANDB_API_KEY\"] = '7c7ab5e2b09dbcf8676c8afe3f110fddc1638ded'\n","\n","# Feel free to change these and experiment !!\n","config = wandb.config\n","config.learning_rate = 2e-5\n","config.batch_size = 32\n","config.output_size = 2\n","config.hidden_size = 256\n","config.embedding_length = 300\n","config.epochs = 10"]},{"cell_type":"markdown","metadata":{"id":"nmI_j69k9i-H"},"source":["# 💿 The Dataset\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"9Xy_k0At-UGT"},"source":["In this code cell we use torchtext legacy module to create a Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1kFBF4DEwwPR","outputId":"51a7dc40-8904-4ebf-a565-074ad7e3371b"},"outputs":[{"name":"stdout","output_type":"stream","text":["downloading aclImdb_v1.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 43.0MB/s]\n",".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n","100%|█████████▉| 399999/400000 [00:52<00:00, 7643.86it/s]\n"]}],"source":["# Ported from: https://github.com/prakashpandey9/Text-Classification-Pytorch/blob/master/load_data.py\n","\n","def load_dataset(test_sen=None):\n","    \n","    tokenize = lambda x: x.split()\n","    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n","    LABEL = data.LabelField()\n","    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n","    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n","    LABEL.build_vocab(train_data)\n","\n","    word_embeddings = TEXT.vocab.vectors\n","\n","    train_data, valid_data = train_data.split()\n","    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), \n","                                                                   batch_size=32, \n","                                                                   sort_key=lambda x: len(x.text), \n","                                                                   repeat=False, shuffle=True)\n","\n","    vocab_size = len(TEXT.vocab)\n","\n","    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter\n","\n","TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()"]},{"cell_type":"markdown","metadata":{"id":"Wuss5ZGQ9r0x"},"source":["# ✍️ Model Architecture\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY7FMji7wycY"},"outputs":[],"source":["class LSTMClassifier(nn.Module):\n","\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n","\t\tsuper(LSTMClassifier, self).__init__()\n","\t\tself.batch_size = batch_size\n","\t\tself.output_size = output_size\n","\t\tself.hidden_size = hidden_size\n","\t\tself.vocab_size = vocab_size\n","\t\tself.embedding_length = embedding_length\n","\t\t\n","\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n","\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) \n","\t\tself.lstm = nn.LSTM(embedding_length, hidden_size) # Our main hero for this tutorial\n","\t\tself.label = nn.Linear(hidden_size, output_size)\n","\t\t\n","\tdef forward(self, input_sentence, batch_size=None):\n","\t\tinput = self.word_embeddings(input_sentence) \n","\t\tinput = input.permute(1, 0, 2) \n","\t\tif batch_size is None:\n","\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) \n","\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) \n","\t\telse:\n","\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n","\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n","\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n","\t\tfinal_output = self.label(final_hidden_state[-1]) \n","\t\t\n","\t\treturn final_output"]},{"cell_type":"markdown","metadata":{"id":"FWRwT28S9y2a"},"source":["# 🧱 + 🏗 = 🏠 Training\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"_OyrH4ta9038"},"source":["## 🥼 Helper Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxlsEhz4w3mT"},"outputs":[],"source":["def clip_gradient(model, clip_value):\n","    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n","    for p in params:\n","        p.grad.data.clamp_(-clip_value, clip_value)\n","    \n","def train_model(model, train_iter, epoch):\n","    total_epoch_loss = 0\n","    total_epoch_acc = 0\n","    model.cuda()\n","    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n","    steps = 0\n","    model.train()\n","    for idx, batch in enumerate(train_iter):\n","        text = batch.text[0]\n","        target = batch.label\n","        target = torch.autograd.Variable(target).long()\n","        if torch.cuda.is_available():\n","            text = text.cuda()\n","            target = target.cuda()\n","        if (text.size()[0] is not 32): \n","            continue\n","        optim.zero_grad()\n","        prediction = model(text)\n","        loss = loss_fn(prediction, target)\n","        wandb.log({\"Training Loss\": loss.item()})\n","        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n","        acc = 100.0 * num_corrects/len(batch)\n","        wandb.log({\"Training Accuracy\": acc.item()})\n","        loss.backward()\n","        clip_gradient(model, 1e-1)\n","        optim.step()\n","        steps += 1\n","        \n","        if steps % 100 == 0:\n","            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n","        \n","        total_epoch_loss += loss.item()\n","        total_epoch_acc += acc.item()\n","        \n","    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n","\n","def eval_model(model, val_iter):\n","    total_epoch_loss = 0\n","    total_epoch_acc = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for idx, batch in enumerate(val_iter):\n","            text = batch.text[0]\n","            if (text.size()[0] is not 32):\n","                continue\n","            target = batch.label\n","            target = torch.autograd.Variable(target).long()\n","            if torch.cuda.is_available():\n","                text = text.cuda()\n","                target = target.cuda()\n","            prediction = model(text)\n","            loss = loss_fn(prediction, target)\n","            wandb.log({\"Evaluation Loss\": loss.item()})\n","            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n","            acc = 100.0 * num_corrects/len(batch)\n","            wandb.log({\"Evaluation Accuracy\": acc.item()})\n","            total_epoch_loss += loss.item()\n","            total_epoch_acc += acc.item()\n","\n","    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"]},{"cell_type":"markdown","metadata":{"id":"BKAA2rR0-B-3"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1XaWIg0w6XZ"},"outputs":[],"source":["model = LSTMClassifier(config.batch_size, config.output_size, config.hidden_size, vocab_size, config.embedding_length, word_embeddings)\n","loss_fn = F.cross_entropy\n","\n","# Create a wandb run to log all your metrics\n","run = wandb.init(project='...', entity='...', reinit=True)\n","\n","wandb.watch(model)\n","\n","for epoch in range(config.epochs):\n","    train_loss, train_acc = train_model(model, train_iter, epoch)\n","    val_loss, val_acc = eval_model(model, valid_iter)\n","    \n","    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n","\n","run.finish()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["nmI_j69k9i-H","Wuss5ZGQ9r0x","FWRwT28S9y2a","_OyrH4ta9038","BKAA2rR0-B-3"],"name":"LSTMs in PyTorch 🔥","provenance":[{"file_id":"https://gist.github.com/SauravMaheshkar/168f0817f0cd29dd4048868fb0dd4401#file-lstms-in-pytorch-ipynb","timestamp":1680843269145}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":0}
